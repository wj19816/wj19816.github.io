---
layout: post
title: '【笔记】大数据分析-分布式计算'
date: 2020-09-17
permalink: /notes/pm/distribute
tags:
  - 研究生
  - 计算机
  - 分布式
latex: true
---



# 课程要求

- 老师：李丰

  - Email: feng.li@cufe.edu.cn 
  - Phone: +86-(0)10-6177-6189

- 教材：[Distributed-Statistical-Computing](https://github.com/feng-li/Distributed-Statistical-Computing)/大数据分布式计算与案例/[新版教材](https://feng.li/files/distcompbook/index.html)

- 结课方式：平时成绩(50%)+期末项目(50%)

- 课程要求：

  - 期末项目要求体现工作量
  - 不允许迟到

```shell
ssh devel@39.98.252.239
cufe1949!

```



# L01-Introduction

> DATE: 2020-09-14



## L01.1-Introduction-to-Distributed-Computing

### 什么是分布式？

分布式与传统的顺序范式编程不同，传统顺序范式按照代码顺序逐行执行，分布式利用多台计算机同时进行执行。

With a distributed system, this same data set will be divided into smaller (typically 64 MB) blocks that are spread among many machines in the cluster via the **Distributed File System**.

- 在超级计算机上，计算机被重新部署：所有内存、CPU、硬盘分别焊接在一起。但成本过高，一般采用data-to-code方式。
- 分布式特指的是将计算分布在许多不同的计算机之间。

分布式计算是一种**move-code-to-data**方式，从而实现并行计算，提高效率。



### MapReduce

> Map：映射，将命令传达到各个计算机进行计算
>
> Reduce：整合，将计算结果进行合并输出



### 分布式计算框架

> 这是一种平台，但不是编程语言

包括：

- 分布式文件储存系统：HDFS
- 分布式计算平台：Spark
- 分布式任务资源管理：ZOOKEEPER

![image](/Users/didi/Desktop/wj19816.github.io/img/hadoop_ecosystem.png)

最基础的底层是HDFS文件存储系统，然后需要一个资源调度器YARN，上面一层是运行框架。



![mapreduce](/Users/didi/Desktop/wj19816.github.io/img/hdfs.png)





### 作业：

`chaoxin 下周二23：55`

1. 寻找计算机某一个硬盘下所有文本文件的文件名包含“win”的文件名，并修改成一个并行程序，记录两者的时间。
2. 思考：分布式与并行计算的区别是什么？

```
# R：parallel
# Python：multiprocessing
```





## L01.2-Linux-Basics

```shell
# 将本机电脑文件上传至服务器+hadoop
scp 本地文件路径 devel@39.98.252.239:/home/devel/students/2020211027wangjing/
hadoop fs -put 本地文件路径 hadoop文件路径 
# 或者
rsync -av 源文件 cufe@192.168.113.164
```

### 作业：

1. 重新更新代码，计算communication time和cultivation time，画出一张曲线图，随着核数的增加，时间的变化。



## L01.3-Introduction-to-Hadoop

java-home
hadoop-home

```shell
echo $JAVA_HOME
echo $HADOOP_HOME
cd /usr/lib/hadoop-current
```

进入hadoop_Home文件夹，得到以下文件夹：

- bin
  - binary的缩写，hadoop中所有的内置命令
- etc
  - hadoop中的配置文件
  - etc/hadoop/：安装hadoop文件的一些必要文件
    - etc/hadoop/hadoop-env.sh：hadoop中重要的环境变量
- include&lib
  - Hadoop执行必要的库
- shell
  - hadoop能被其他程序调用的模块



```shell
#可以看做一个单机版本的MapReduce
cat LICENSE.txt | wc 

#hadoop版本的MapReduce
hadoop fs -rm -r 2020211027wangjing/output
hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.1.3.jar -input 2020211027wangjing/LICENSE.txt -output 2020211027wangjing/output -mapper "/usr/bin/cat" -reducer "/usr/bin/wc"


#查看hadoop-streaming版本
ls /usr/lib/hadoop-current/share/hadoop/tools/lib   
#hadoop-streaming-3.1.3.jar

#在自己文件夹内新建一个可以使用的文本文件并上传hadoop
touch LICENSE.txt
vim LICENSE.txt
hadoop fs -put LICENSE.txt 文件夹/LICENSE.txt

#run
hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.1.3.jar -input 文件夹/LICENSE.txt -output 文件夹/output -mapper "/usr/bin/cat" -reducer "/usr/bin/wc"
```





`2020年10月8日`

```linux
#一些快捷键

ctrl+a #光标移至行首
ctrl+e #光标移至行尾
ctrl+l #清屏
ctrl+k #剪切所有光标后面的内容
```



## L01.4 Map-Reduce

虽然是分布式系统，但数据存在HDFS的硬盘hard disk上，所以会有I/O消耗。

Map也可能做复杂计算，中间计算结果也需要储存到硬盘，Reduce读取中间计算结果，这两步也需要I/O时间。

好处：节省了计算资源。

硬盘做成SSD更快，但是更容易快，因为hadoop作冗余。



### 标准输入输出：std-in/std-out

> 例如：
>
> 标准输入：linux-管道；open



hadoop一开始是用java写的，java特别适合大型项目。

但是java会的人少。因此hadoop提出了streaming来识别其他计算机语言（只要支持标准输入输出即可）。



```python
#! /usr/bin/env python3

#手写一个wc.py文件
#第一行是指定python3路径

import  sys
count = 0
data = []
for line in sys.stdin:
  count += 1
  data.append(line)
print(count) #print goes to sys.stdout  

```



```shell
cd /home/devel/students/2020211027wangjing
touch wc.py
vim wc.py
#粘贴py代码==>esc==>:wq退出编辑
hadoop fs -put LICENSE.txt 2020211027wangjing/LICENSE.txt

#run
hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.1.3.jar \
-input 2020211027wangjing/LICENSE.txt \
-output 2020211027wangjing/output2 \
-file "wc.py" \
-mapper "/usr/bin/cat" \
-reducer "wc.py"


hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.1.3.jar \
    -input ${HADOOP_INPUT_DIR} \
    -output ${HADOOP_OUTPUT_DIR} \
    -file "wc.py" \
    -mapper "/usr/bin/cat" \
    -reducer "wc.py"
```



作业：

https://github.com/feng-li/Distributed-Statistical-Computing/tree/master/L02-MapReduce/examples

自己找一个小的数据，对一个单一任务，用不同的MapReduce来实现。（python、bash、R等）

用main.sh来实现以下功能：

- bash+python



-file :指定运行文件，将wc.py传到每一个worker上去，然后再进行MapReduce



- 小技巧：

```shell
screen -DR wangjing #相当于浏览器多标签功能，开启一个窗口
ctrl+a c  #创建一个新creen
ctrl+a n  #切换下一个screen
ctrl+a d  #退出该屏幕
```





## MapReduce

Key/value：

map之后的文件会存在【交换分区】中。



# L02-Spark

Hadoop不适合做的东西：

- 实时计算
- 需要迭代：梯度下降等机器学习算法

因此更适合用Spark。但是Spark贵很多。



# L04-Hive

```bash
# 用terminal运行hive命令
hive -e "dfs -ls /;"

# 用文件运行hive命令
hive -f /path/to/file/withqueries.hql
```



```sql
show databases;
-- 新建数据库
create database if not exists wangjing location '/home/devel/students/2020211027wangjing/hive'; 

drop database wangjing;

-- 新建数据表
create table if not exists wangjing.employee(
  	name STRING comment 'Employee name',
    salery FLOAT comment 'Employee salery',
    subordinates ARRAY<STRING> COMMENT 'Names of subordinates',
    deductions MAP<STRING, FLOAT> comment 'Keys are deductions names, values are percentages',
    address STRUCT<street:STRING, city:STRING, state:STRING, zip:INT> comment 'Home address'
)
comment 'Description of the table'
TBLPROPERTIES ('creator'='me', 'created_at'='2012-01-02 10:00:00');


-- 从csv文件中导入数据
drop table if exists wangjing.titanic;
create external table if not exists wangjing.titanic (
	PassengerId STRING,
  Survived INT,
  Pclass INT,
  Sex STRING,
  Age INT,
  SibSp INT,
  Parch INT,
  Ticket STRING,
  Fare FLOAT,
  Cabin STRING,
  Embarked STRING
)
row format delimited fields terminated by ',' 
location '/home/devel/students/2020211027wangjing/data' 
--location意味着这个文件夹下的所有数据都会被读取进入这个表
TBLPROPERTIES('skip.header.line.count'='1'); --可以跳过第一行标题或最后几行

```



## Spark

```shell



PYSPARK_PYTHON=python3.6 spark-submit\
   --master yarn \
   spark-clean.py > out.txt
```



- lazzy evaluation => dag：有向无环图





1. 看看spark的矩阵
2. 模拟回归数据



```python
#模拟
import numpy as np
mu, sigma = 2, 0.5

```







```python
from pyspark.sql.types import *

# 设置数据格式
schema_sdf = StructType([
        StructField('Year', IntegerType(), True),
        StructField('Month', IntegerType(), True),
        StructField('DayofMonth', IntegerType(), True),
        StructField('DayOfWeek', IntegerType(), True),
        StructField('DepTime', DoubleType(), True),
        StructField('CRSDepTime', DoubleType(), True),
        StructField('ArrTime', DoubleType(), True),
        StructField('CRSArrTime', DoubleType(), True),
        StructField('UniqueCarrier', StringType(), True),
        StructField('FlightNum', StringType(), True),
        StructField('TailNum', StringType(), True),
        StructField('ActualElapsedTime', DoubleType(), True),
        StructField('CRSElapsedTime',  DoubleType(), True),
        StructField('AirTime',  DoubleType(), True),
        StructField('ArrDelay',  DoubleType(), True),
        StructField('DepDelay',  DoubleType(), True),
        StructField('Origin', StringType(), True),
        StructField('Dest',  StringType(), True),
        StructField('Distance',  DoubleType(), True),
        StructField('TaxiIn',  DoubleType(), True),
        StructField('TaxiOut',  DoubleType(), True),
        StructField('Cancelled',  IntegerType(), True),
        StructField('CancellationCode',  StringType(), True),
        StructField('Diverted',  IntegerType(), True),
        StructField('CarrierDelay', DoubleType(), True),
        StructField('WeatherDelay',  DoubleType(), True),
        StructField('NASDelay',  DoubleType(), True),
        StructField('SecurityDelay',  DoubleType(), True),
        StructField('LateAircraftDelay',  DoubleType(), True)
    ])

#读入数据
air = spark.read.options(header='true').schema(schema_sdf).csv("/data/airdelay_small.csv")

#描述统计
air.describe().show()
air.describe(['ArrDelay']).show()

#查看数据集前n条
air.select(["ArrDelay","AirTime","Distance"]).show()

#数据筛选
air.select(air['UniqueCarrier'], air['ArrDelay']>0).show()

#数据分组
air.groupBy(["UniqueCarrier","DayOfWeek"]).count().show()

#数据排序
aircount=air.groupBy("UniqueCarrier").count()
aircount.sort("count",ascending=False).show()

#去除缺失值
air_without_na = air.na.drop()
air_without_na.show()

usecols_x = [
  'Year',
  'Month',
  'DayofMonth',
  'DayOfWeek',
  'DepTime',
  'CRSDepTime',
  'CRSArrTime',
  'UniqueCarrier',
  'ActualElapsedTime',
  'Origin',
  'Dest',
  'Distance'
]
```



作业：

通过spark中的dataframe的操作，进行数据预处理

1. 缺失值处理等

    - 原始表： 500w*19

    - y：airdelay  ，飞机是否延误
    - x：carryier

1. 创建成一个新的表：
   - 可能是500w*180
   - y：airdelay ：0，1
   - 数值型变量可以保留，分类变量增加哑变量（is_xx=0，1）
     - 哑变量需要计算缺失比例
     - 85%以上作为其他



```python
'''
思路:
	1. 

一些小技巧：
	- pipeline
	
'''

from pyspark.sql.types import *

# 设置数据格式
schema_sdf = StructType([
        StructField('Year', IntegerType(), True),
        StructField('Month', IntegerType(), True),
        StructField('DayofMonth', IntegerType(), True),
        StructField('DayOfWeek', IntegerType(), True),
        StructField('DepTime', DoubleType(), True),
        StructField('CRSDepTime', DoubleType(), True),
        StructField('ArrTime', DoubleType(), True),
        StructField('CRSArrTime', DoubleType(), True),
        StructField('UniqueCarrier', StringType(), True),
        StructField('FlightNum', StringType(), True),
        StructField('TailNum', StringType(), True),
        StructField('ActualElapsedTime', DoubleType(), True),
        StructField('CRSElapsedTime',  DoubleType(), True),
        StructField('AirTime',  DoubleType(), True),
        StructField('ArrDelay',  DoubleType(), True),
        StructField('DepDelay',  DoubleType(), True),
        StructField('Origin', StringType(), True),
        StructField('Dest',  StringType(), True),
        StructField('Distance',  DoubleType(), True),
        StructField('TaxiIn',  DoubleType(), True),
        StructField('TaxiOut',  DoubleType(), True),
        StructField('Cancelled',  IntegerType(), True),
        StructField('CancellationCode',  StringType(), True),
        StructField('Diverted',  IntegerType(), True),
        StructField('CarrierDelay', DoubleType(), True),
        StructField('WeatherDelay',  DoubleType(), True),
        StructField('NASDelay',  DoubleType(), True),
        StructField('SecurityDelay',  DoubleType(), True),
        StructField('LateAircraftDelay',  DoubleType(), True)
    ])

#读入数据
air = spark.read.options(header='true').schema(schema_sdf).csv("/data/airdelay_small.csv")

usecols_y =  ['ArrDelay']
usecols_x = [
  'Year',          #年      [1987,2007]
  'Month',         #月      [1,12]
  'DayofMonth',    #日      [1,31]
  'DayOfWeek',     #星期     [1,7]
  'DepTime',       #起飞时间 [1,2644]
  'CRSDepTime',    #计划起飞时间  [0,2400]
  'CRSArrTime',    #计划到达时间  [0,2400]
  'UniqueCarrier',   #航空公司
  'ActualElapsedTime',  #实际飞行时间 [-681,1766]
  'Origin',        #起点
  'Dest',          #终点
  'Distance'       #距离[11,4983]
]

#查看选中列的情况且描述统计
air_m = air.select(usecols_y+usecols_x)
air_m.show()
air_m.describe().show()

#剔除缺失值
air_m_without_na = air_m.dropna(how='any')
air_m_without_na.count()  #5548754 ==> 5423403

#

#创建数据集的sql表
#air_m.createOrReplaceTempView('air_data')
#删除临时表
#spark.catalog.dropTempView("air_data")

#ArrDelay：大于0记为1

```

![image-20201112181842890](/Users/didi/Library/Application Support/typora-user-images/image-20201112181842890.png)



缺失比例：

![image-20201115202526114](/Users/didi/Library/Application Support/typora-user-images/image-20201115202526114.png)





```python
#期末作业：

参数配置参考：
https://blog.csdn.net/pipisorry/article/details/52912179
```

