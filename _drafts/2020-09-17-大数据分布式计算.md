---

layout: post
title: '【笔记】大数据分析-分布式计算'
date: 2020-09-17
permalink: /notes/pm/distribute
tags:
  - 研究生
  - 计算机
  - 分布式
latex: true
---



# 课程要求

- 老师：李丰

  - Email: feng.li@cufe.edu.cn 
  - Phone: +86-(0)10-6177-6189

- 教材：[Distributed-Statistical-Computing](https://github.com/feng-li/Distributed-Statistical-Computing)/大数据分布式计算与案例/[新版教材](https://feng.li/files/distcompbook/index.html)

- 结课方式：平时成绩(50%)+期末项目(50%)

- 课程要求：

  - 期末项目要求体现工作量
  - 不允许迟到

```shell
ssh devel@39.98.252.239
cufe1949!
```



# L01-Introduction

> DATE: 2020-09-14



## L01.1-Introduction-to-Distributed-Computing

### 什么是分布式？

分布式与传统的顺序范式编程不同，传统顺序范式按照代码顺序逐行执行，分布式利用多台计算机同时进行执行。

With a distributed system, this same data set will be divided into smaller (typically 64 MB) blocks that are spread among many machines in the cluster via the **Distributed File System**.

- 在超级计算机上，计算机被重新部署：所有内存、CPU、硬盘分别焊接在一起。但成本过高，一般采用data-to-code方式。
- 分布式特指的是将计算分布在许多不同的计算机之间。

分布式计算是一种**move-code-to-data**方式，从而实现并行计算，提高效率。



### MapReduce

> Map：映射，将命令传达到各个计算机进行计算
>
> Reduce：整合，将计算结果进行合并输出



### 分布式计算框架

> 这是一种平台，但不是编程语言

包括：

- 分布式文件储存系统：HDFS
- 分布式计算平台：Spark
- 分布式任务资源管理：ZOOKEEPER

![image](/Users/didi/Desktop/wj19816.github.io/img/hadoop_ecosystem.png)

最基础的底层是HDFS文件存储系统，然后需要一个资源调度器YARN，上面一层是运行框架。



![mapreduce](/Users/didi/Desktop/wj19816.github.io/img/hdfs.png)





### 作业：

`chaoxin 下周二23：55`

1. 寻找计算机某一个硬盘下所有文本文件的文件名包含“win”的文件名，并修改成一个并行程序，记录两者的时间。
2. 思考：分布式与并行计算的区别是什么？

```
# R：parallel
# Python：multiprocessing
```





## L01.2-Linux-Basics

```shell
# 将本机电脑文件上传至服务器+hadoop
scp 本地文件路径 devel@39.98.252.239:/home/devel/students/文件夹/文件名
hadoop fs -put 本地文件路径 hadoop文件路径
# 或者
rsync -av 源文件 cufe@192.168.113.164
```

### 作业：

1. 重新更新代码，计算communication time和cultivation time，画出一张曲线图，随着核数的增加，时间的变化。



## L01.3-Introduction-to-Hadoop

java-home
hadoop-home

```shell
echo $JAVA_HOME
echo $HADOOP_HOME
cd /usr/lib/hadoop-current
```

进入hadoop_Home文件夹，得到以下文件夹：

- bin
  - binary的缩写，hadoop中所有的内置命令
- etc
  - hadoop中的配置文件
  - etc/hadoop/：安装hadoop文件的一些必要文件
    - etc/hadoop/hadoop-env.sh：hadoop中重要的环境变量
- include&lib
  - Hadoop执行必要的库
- shell
  - hadoop能被其他程序调用的模块



```shell
#可以看做一个单机版本的MapReduce
cat LICENSE.txt | wc 

#hadoop版本的MapReduce
hadoop fs -rm -r 2020211027wangjing/output
hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.1.3.jar -input 2020211027wangjing/LICENSE.txt -output 2020211027wangjing/output -mapper "/usr/bin/cat" -reducer "/usr/bin/wc"


#查看hadoop-streaming版本
ls /usr/lib/hadoop-current/share/hadoop/tools/lib   
#hadoop-streaming-3.1.3.jar

#在自己文件夹内新建一个可以使用的文本文件并上传hadoop
touch LICENSE.txt
vim LICENSE.txt
hadoop fs -put LICENSE.txt 文件夹/LICENSE.txt

#run
hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.1.3.jar -input 文件夹/LICENSE.txt -output 文件夹/output -mapper "/usr/bin/cat" -reducer "/usr/bin/wc"
```





`2020年10月8日`

```linux
#一些快捷键

ctrl+a #光标移至行首
ctrl+e #光标移至行尾
ctrl+l #清屏
ctrl+k #剪切所有光标后面的内容
```



## L01.4 Map-Reduce

虽然是分布式系统，但数据存在HDFS的硬盘hard disk上，所以会有I/O消耗。

Map也可能做复杂计算，中间计算结果也需要储存到硬盘，Reduce读取中间计算结果，这两步也需要I/O时间。

好处：节省了计算资源。

硬盘做成SSD更快，但是更容易快，因为hadoop作冗余。



### 标准输入输出：std-in/std-out

> 例如：
>
> 标准输入：linux-管道；open



hadoop一开始是用java写的，java特别适合大型项目。

但是java会的人少。因此hadoop提出了streaming来识别其他计算机语言（只要支持标准输入输出即可）。



```python
#! /usr/bin/env python3

#手写一个wc.py文件
#第一行是指定python3路径

import  sys
count = 0
data = []
for line in sys.stdin:
  count += 1
  data.append(line)
print(count) #print goes to sys.stdout  

```



```shell
cd /home/devel/students/2020211027wangjing
touch wc.py
vim wc.py
#粘贴py代码==>esc==>:wq退出编辑
hadoop fs -put LICENSE.txt 2020211027wangjing/LICENSE.txt

#run
hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.1.3.jar \
-input 2020211027wangjing/LICENSE.txt \
-output 2020211027wangjing/output2 \
-file "wc.py" \
-mapper "/usr/bin/cat" \
-reducer "wc.py"


hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.1.3.jar \
    -input ${HADOOP_INPUT_DIR} \
    -output ${HADOOP_OUTPUT_DIR} \
    -file "wc.py" \
    -mapper "/usr/bin/cat" \
    -reducer "wc.py"
```



作业：

https://github.com/feng-li/Distributed-Statistical-Computing/tree/master/L02-MapReduce/examples

自己找一个小的数据，对一个单一任务，用不同的MapReduce来实现。（python、bash、R等）

用main.sh来实现以下功能：

- bash+python
- 



-file :指定运行文件，将wc.py传到每一个worker上去，然后再进行MapReduce



# L02-Spark

Hadoop不适合做的东西：

- 实时计算
- 需要迭代：梯度下降等机器学习算法

因此更适合用Spark。但是Spark贵很多。